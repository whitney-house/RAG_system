---
title: {{title}}
emoji: {{emoji}}
colorFrom: {{colorFrom}}
colorTo: {{colorTo}}
sdk: {{sdk}}
sdk_version: "{{sdkVersion}}"
app_file: app.py
pinned: false
---


# Recipe RAG System  
Inspired by course: [Understanding LLMs](https://cogsciprag.github.io/Understanding-LLMs-course/intro.html)  

## Project Overview  
This project is a cutting-edge **Retrieval-Augmented Generation (RAG)** system designed for **recipe retrieval** and **context-aware response generation**.  
It integrates **Llama** as the core language model, **MiniLM embeddings** for efficient retrieval, and **FAISS** for vector search, all exposed through a high-performance **FastAPI server**.  

‚úÖ **Tested on MacBook Air M1** ‚Äî Runs smoothly with efficient memory usage!  
‚úÖ **Easily adaptable** to other domains like **automotive repair, industrial manufacturing, or medical Q&A** ‚Äî just swap datasets and models.  

---

## Key Features  
- **Advanced RAG Architecture** ‚Äì Llama LLM + MiniLM embeddings + FAISS retrieval  
- **FastAPI-Powered API** ‚Äì Low-latency, production-ready RESTful service  
- **Fully Modular** ‚Äì Users can easily swap models, embeddings, and datasets  
- **Optimized for Apple Silicon** ‚Äì Runs smoothly on **MacBook Air M1** without GPU  
- **Dockerized Deployment** ‚Äì Ready to deploy with **Docker & Docker Compose**  
- **React Frontend** ‚Äì Chat-based UI for recipe exploration  

---

## Technology Stack  

| **Component**      | **Technology**                                    |
|--------------------|--------------------------------------------------|
| **Language Model** | Llama (Meta AI)                                  |
| **Vector Search**  | FAISS + LlamaIndex                               |
| **Embeddings**     | `sentence-transformers/all-MiniLM-L6-v2`         |
| **API Framework**  | FastAPI                                          |
| **Frontend**      | JavaScript, React.js                             |
| **Database**      | FAISS for scalable vector search                 |
| **Containerization** | Docker & Docker Compose                       |

---

## System Architecture  

### 1Ô∏è‚É£ Backend (RAG Core + API Server)  
- **Retrieval**: FAISS + MiniLM embeddings for fast **semantic search**  
- **Generation**: Llama model generates responses with retrieved **context**  
- **API Interface**: FastAPI exposes endpoints for easy **integration**  

### 2Ô∏è‚É£ Frontend (React UI)  
- **Conversational chat interface**  
- **Displays AI-generated recipes and contextual responses**  

---

## Demo Preview  
Below are sample screenshots showcasing the system in action. The images are stored in the `assets/` folder:  

| **Demo**                  | **Description**                          |
|---------------------------|------------------------------------------|
| ![Demo 1](https://github.com/whitney-house/RAG_system/blob/main/fronted/src/assets/demo1.png) | Design          |
| ![Demo 2](https://github.com/whitney-house/RAG_system/blob/main/fronted/src/assets/demo2.png) | Recipe retrieval in action               |
| ![Demo 3](https://github.com/whitney-house/RAG_system/blob/main/fronted/src/assets/demo3.png) | Personalized recipe recommendations   |

---


## Future Improvements
‚úÖ Agent Integration ‚Äì Adding AI Agents for more complex reasoning and interactions  
‚úÖ Cross-Domain Support ‚Äì Expand to industrial, automotive, and healthcare applications  
‚úÖ Fine-Tuning Support ‚Äì Enable custom Llama training for domain-specific use cases  
‚úÖ Enhanced Vector Search ‚Äì Move beyond FAISS to more scalable solutions  

## License & Usage
This project is open for customization‚Äîfeel free to replace models and datasets for your own use.  
**‚ö†Ô∏è Do not use this for commercial purposes without permission. üöÄ**

